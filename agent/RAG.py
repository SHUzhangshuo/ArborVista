"""
è®ºæ–‡æ•°æ®åº“RAGæ£€ç´¢ç³»ç»Ÿ
åŸºäºLangChainå®ç°è®ºæ–‡æ–‡æ¡£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    # Fallback to deprecated version
    from langchain_community.embeddings import HuggingFaceEmbeddings

# Import config for RAG settings
try:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from app.config import Config
except ImportError:
    # Fallback if config not available
    Config = None


class PaperRAGSystem:
    """è®ºæ–‡RAGæ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(
        self,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        vector_store_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            base_url: LLM APIåŸºç¡€URLï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–ï¼‰
            api_key: APIå¯†é’¥ï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä»ç¯å¢ƒå˜é‡æˆ–é…ç½®è¯»å–ï¼‰
            vector_store_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        """
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è¯»å–å‚æ•°
        if Config:
            self.base_url = base_url or Config.OPENAI_BASE_URL or None
            self.api_key = api_key or Config.OPENAI_API_KEY
            self.model = model or Config.OPENAI_MODEL
            self.temperature = temperature if temperature is not None else Config.OPENAI_TEMPERATURE
        else:
            # Fallback to environment variables
            self.base_url = base_url or os.environ.get('OPENAI_BASE_URL') or None
            self.api_key = api_key or os.environ.get('OPENAI_API_KEY')
            self.model = model or os.environ.get('OPENAI_MODEL', 'gpt-5')
            self.temperature = temperature if temperature is not None else float(os.environ.get('OPENAI_TEMPERATURE', '0.7'))
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if not self.api_key:
            raise ValueError(
                "OPENAI_API_KEY æœªè®¾ç½®ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®ã€‚"
            )
        
        # éªŒè¯ base_urlï¼ˆå¦‚æœæœªè®¾ç½®ï¼ŒæŠ›å‡ºé”™è¯¯è€Œä¸æ˜¯ä½¿ç”¨ç¡¬ç¼–ç å€¼ï¼‰
        if not self.base_url:
            raise ValueError(
                "OPENAI_BASE_URL æœªè®¾ç½®ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_BASE_URL æˆ–åœ¨é…ç½®æ–‡ä»¶ä¸­é…ç½®ã€‚"
            )
        
        # è®¾ç½®å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
        if vector_store_path is None:
            # ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ data/vectorDatabase
            base_dir = Path(__file__).parent.parent
            vector_store_path = str(base_dir / "data" / "vectorDatabase")
        
        self.vector_store_path = Path(vector_store_path)
        self.vector_store_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–LLMï¼ˆä½¿ç”¨å·²ç»å¤„ç†å¥½çš„ self å±æ€§ï¼‰
        self.llm = ChatOpenAI(
            model=self.model,
            base_url=self.base_url,
            api_key=self.api_key,
            temperature=self.temperature
        )
        
        # Initialize Embeddings
        # Use local HuggingFace model, supports Chinese and English
        # Model will be saved to vectorDatabase/models directory
        try:
            # Create model cache directory
            model_cache_dir = self.vector_store_path / "models"
            model_cache_dir.mkdir(parents=True, exist_ok=True)
            
            # Set environment variables to specify model cache directory
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_cache_dir)
            os.environ['HF_HOME'] = str(model_cache_dir)
            
            model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            
            from sentence_transformers import SentenceTransformer
            
            # Pre-load model to ensure proper loading and avoid meta tensor issues
            # Auto-detect GPU if available
            try:
                import torch
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                if device == 'cuda':
                    print(f"âœ… æ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CUDA åŠ é€Ÿ: {torch.cuda.get_device_name(0)}")
                else:
                    print("â„¹ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU")
            except ImportError:
                device = 'cpu'
                print("â„¹ï¸  PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨ CPU")
            
            try:
                st_model = SentenceTransformer(
                    model_name,
                    cache_folder=str(model_cache_dir),
                    device=device
                )
                _ = st_model.encode("test", normalize_embeddings=True)
            except Exception as e:
                raise RuntimeError(
                    f"Failed to load embedding model from {model_cache_dir}. "
                    f"Please ensure the model is downloaded correctly. Error: {str(e)}"
                ) from e
            
            # Create HuggingFaceEmbeddings wrapper
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=model_name,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                test_embedding = self.embeddings.embed_query("test")
                if test_embedding is None or len(test_embedding) == 0:
                    raise ValueError("Embedding test failed, returned empty vector")
            except Exception as e:
                raise RuntimeError(
                    f"Failed to create HuggingFaceEmbeddings wrapper. "
                    f"Model loaded successfully but wrapper failed. Error: {str(e)}"
                ) from e
        except ImportError as e:
            print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“")
            print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            print("   è¯·å®‰è£…: pip install sentence-transformers")
            self.embeddings = None
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åˆå§‹åŒ–Embeddings")
            print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
            print("   è¯·ç¡®ä¿å·²å®‰è£…: pip install sentence-transformers")
            print("   å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç£ç›˜ç©ºé—´")
            import traceback
            traceback.print_exc()
            self.embeddings = None
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # æ¯ä¸ªchunkçš„å¤§å°
            chunk_overlap=200,  # chunkä¹‹é—´çš„é‡å 
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        
        # å‘é‡æ•°æ®åº“
        self.vector_store: Optional[FAISS] = None
        
        # æ–‡æ¡£å…ƒæ•°æ®
        self.doc_metadata: Dict[str, Dict] = {}
    
    def _extract_filename(self, file_dir: Path) -> str:
        """Extract filename from file directory"""
        # Try metadata.json
        metadata_file = file_dir / "metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    if filename := metadata.get('original_filename'):
                        return filename
            except:
                pass
        
        # Try filename_info.json
        filename_info_file = file_dir / "filename_info.json"
        if filename_info_file.exists():
            try:
                with open(filename_info_file, 'r', encoding='utf-8') as f:
                    filename_info = json.load(f)
                    if filename := filename_info.get('original_filename'):
                        return filename
            except:
                pass
        
        # Extract from directory name
        dir_name = file_dir.name
        if '-' in dir_name:
            # Try PDF format: filename.pdf-uuid
            parts = dir_name.rsplit('.pdf-', 1)
            if len(parts) == 2:
                return parts[0] + '.pdf'
            # Try other extensions
            for ext in ['.md', '.txt', '.docx', '.doc']:
                if ext in dir_name:
                    parts = dir_name.rsplit(ext + '-', 1)
                    if len(parts) == 2:
                        return parts[0] + ext
            # Use longest part as filename
            return max(dir_name.split('-'), key=len)
        return dir_name
    
    def list_libraries(self) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡åº“
        
        Returns:
            æ–‡åº“åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡åº“åŒ…å«idã€nameã€display_nameç­‰ä¿¡æ¯
        """
        base_dir = Path(__file__).parent.parent
        libraries_dir = base_dir / "data" / "output" / "libraries"
        
        libraries = []
        
        if not libraries_dir.exists():
            print(f"âš ï¸ æ–‡åº“ç›®å½•ä¸å­˜åœ¨: {libraries_dir}")
            return libraries
        
        # éå†æ‰€æœ‰æ–‡åº“ç›®å½•
        for library_dir in libraries_dir.iterdir():
            if not library_dir.is_dir():
                continue
            
            library_id = library_dir.name
            
            # è¯»å–æ–‡åº“ä¿¡æ¯
            library_info = {
                'id': library_id,
                'name': library_id,
                'display_name': library_id,
                'description': ''
            }
            
            info_file = library_dir / "info.json"
            if info_file.exists():
                try:
                    with open(info_file, 'r', encoding='utf-8') as f:
                        library_info = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ è¯»å–æ–‡åº“ä¿¡æ¯å¤±è´¥ {library_id}: {str(e)}")
            
            libraries.append(library_info)
        
        return libraries
    
    def load_papers_from_library(self, library_id: str = "default") -> List[Dict]:
        """
        ä»æ–‡åº“ä¸­åŠ è½½è®ºæ–‡
        
        Args:
            library_id: æ–‡åº“ID
            
        Returns:
            è®ºæ–‡æ–‡æ¡£åˆ—è¡¨
        """
        base_dir = Path(__file__).parent.parent
        library_dir = base_dir / "data" / "output" / "libraries" / library_id
        
        if not library_dir.exists():
            print(f"âš ï¸ æ–‡åº“ç›®å½•ä¸å­˜åœ¨: {library_dir}")
            return []
        
        # è¯»å–æ–‡åº“ä¿¡æ¯
        library_info = {
            'id': library_id,
            'name': library_id,
            'display_name': library_id,
            'description': ''
        }
        info_file = library_dir / "info.json"
        if info_file.exists():
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    library_info = json.load(f)
                    print(f"ğŸ“š åŠ è½½æ–‡åº“: {library_info.get('display_name', library_id)}")
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡åº“ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        papers = []
        
        # éå†æ–‡åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶ç›®å½•
        for file_dir in library_dir.iterdir():
            if not file_dir.is_dir() or file_dir.name == "info.json":
                continue
            
            # æŸ¥æ‰¾full.mdæ–‡ä»¶ï¼ˆå¯èƒ½åœ¨å­ç›®å½•ä¸­ï¼‰
            md_file = file_dir / "full.md"
            actual_file_dir = file_dir
            
            # å¦‚æœå½“å‰ç›®å½•æ²¡æœ‰full.mdï¼Œæ£€æŸ¥å­ç›®å½•
            if not md_file.exists():
                for sub_dir in file_dir.iterdir():
                    if sub_dir.is_dir():
                        potential_md = sub_dir / "full.md"
                        if potential_md.exists():
                            md_file = potential_md
                            actual_file_dir = sub_dir
                            break
                
                if not md_file.exists():
                    continue
            
            # è¯»å–Markdownå†…å®¹
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                filename = self._extract_filename(actual_file_dir) or "æœªå‘½åæ–‡æ¡£"
                
                # ä»filename_info.jsonè·å–æ­£ç¡®çš„file_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»ç›®å½•åæå–
                file_id = None
                filename_info_file = actual_file_dir / "filename_info.json"
                if filename_info_file.exists():
                    try:
                        with open(filename_info_file, 'r', encoding='utf-8') as f:
                            file_info = json.load(f)
                            file_id = file_info.get('file_id')
                    except:
                        pass
                
                # å¦‚æœæ— æ³•ä»filename_info.jsonè·å–ï¼Œä»ç›®å½•åæå–
                if not file_id:
                    dir_name = file_dir.name
                    # å¦‚æœç›®å½•ååŒ…å«_b1åç¼€ï¼Œå»æ‰å®ƒ
                    if dir_name.endswith('_b1'):
                        # å°è¯•ä»ç›®å½•åæå–file_idï¼ˆæ ¼å¼ï¼š{æ–‡ä»¶å}-{file_id} æˆ– {file_id}_b1ï¼‰
                        if '-' in dir_name:
                            # æ ¼å¼ï¼š{æ–‡ä»¶å}-{file_id}
                            file_id = dir_name.rsplit('-', 1)[-1]
                        else:
                            # æ ¼å¼ï¼š{file_id}_b1
                            file_id = dir_name.replace('_b1', '')
                    else:
                        # å¦‚æœç›®å½•ååŒ…å«-ï¼Œæå–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºfile_id
                        if '-' in dir_name:
                            file_id = dir_name.rsplit('-', 1)[-1]
                        else:
                            file_id = dir_name
                
                papers.append({
                    'file_id': file_id,
                    'library_id': library_id,
                    'library_name': library_info.get('display_name', library_id),
                    'filename': filename,
                    'content': content,
                    'path': str(md_file)
                })
                
                print(f"âœ… åŠ è½½è®ºæ–‡: {filename} (file_id: {file_id}, dir: {file_dir.name})")
                
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è®ºæ–‡å¤±è´¥: {file_dir.name}, é”™è¯¯: {str(e)}")
                continue
        
        print(f"ğŸ“š å…±åŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
        return papers
    
    def build_vector_store(self, papers: List[Dict], library_id: str = "default") -> bool:
        """
        æ„å»ºå‘é‡æ•°æ®åº“
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            library_id: æ–‡åº“ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.embeddings:
            print("âŒ Embeddingsæœªåˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºå‘é‡æ•°æ®åº“")
            return False
        
        if not papers:
            print("âš ï¸ æ²¡æœ‰è®ºæ–‡å¯å¤„ç†")
            return False
        
        # Clear existing vector store and metadata before building new one
        # This ensures we don't mix data from different libraries
        self.vector_store = None
        
        # Use local metadata dictionary to avoid being overwritten by concurrent requests
        # Only update self.doc_metadata after successful save
        local_metadata = {}
        all_documents = []
        
        # å¤„ç†æ¯ç¯‡è®ºæ–‡
        for paper in papers:
            content = paper['content']
            file_id = paper['file_id']
            filename = paper['filename']
            library_name = paper.get('library_name', library_id)
            
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.text_splitter.split_text(content)
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            for i, chunk in enumerate(chunks):
                from langchain_core.documents import Document
                doc = Document(
                    page_content=chunk,
                    metadata={
                        'file_id': file_id,
                        'library_id': library_id,
                        'library_name': library_name,
                        'filename': filename,
                        'chunk_index': i,
                        'total_chunks': len(chunks)
                    }
                )
                all_documents.append(doc)
            
            # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®åˆ°å±€éƒ¨å˜é‡
            local_metadata[file_id] = {
                'filename': filename,
                'library_id': library_id,
                'library_name': library_name,
                'chunk_count': len(chunks)
            }
        
        print(f"ğŸ“ å…±ç”Ÿæˆ {len(all_documents)} ä¸ªæ–‡æœ¬å—")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        try:
            print("ğŸ”„ æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“...")
            # Create vector store in local variable first
            vector_store = FAISS.from_documents(all_documents, self.embeddings)
            
            # ä¿å­˜å‘é‡æ•°æ®åº“
            store_path = self.vector_store_path / f"{library_id}_faiss"
            vector_store.save_local(str(store_path))
            print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {store_path}")
            
            # ä¿å­˜å…ƒæ•°æ®ï¼ˆä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œé¿å…è¢«å¹¶å‘è¯·æ±‚è¦†ç›–ï¼‰
            metadata_path = self.vector_store_path / f"{library_id}_metadata.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(local_metadata, f, ensure_ascii=False, indent=2)
            
            # Only update instance variables after successful save
            # This ensures data consistency
            self.vector_store = vector_store
            self.doc_metadata = local_metadata
            
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def check_vector_store_exists(self, library_id: str = "default") -> tuple[bool, int]:
        """
        æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨ï¼Œä¸å®é™…åŠ è½½
        
        Args:
            library_id: æ–‡åº“ID
            
        Returns:
            (æ˜¯å¦å­˜åœ¨, è®ºæ–‡æ•°é‡)
        """
        store_path = self.vector_store_path / f"{library_id}_faiss"
        
        if not store_path.exists():
            return False, 0
        
        # Check metadata file to get paper count
        metadata_path = self.vector_store_path / f"{library_id}_metadata.json"
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    paper_count = len(metadata) if isinstance(metadata, dict) else 0
                    return True, paper_count
            except Exception:
                return True, 0
        
        return True, 0
    
    def load_vector_store(self, library_id: str = "default") -> bool:
        """
        åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        
        Args:
            library_id: æ–‡åº“ID
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.embeddings:
            print("âŒ Embeddingsæœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½å‘é‡æ•°æ®åº“")
            return False
        
        store_path = self.vector_store_path / f"{library_id}_faiss"
        
        if not store_path.exists():
            print(f"âš ï¸ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨: {store_path}")
            return False
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {store_path}")
            # Clear existing vector store and metadata before loading new one
            self.vector_store = None
            self.doc_metadata = {}
            
            self.vector_store = FAISS.load_local(
                str(store_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = self.vector_store_path / f"{library_id}_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    self.doc_metadata = json.load(f)
            
            print(f"âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.doc_metadata)} ç¯‡è®ºæ–‡")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def create_rag_chain(self, k: int = 4, file_id: Optional[str] = None):
        """
        åˆ›å»ºRAGæ£€ç´¢é“¾
        
        Args:
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            file_id: å¦‚æœæŒ‡å®šï¼Œåªæ£€ç´¢è¯¥æ–‡ä»¶çš„å†…å®¹ï¼ˆNoneè¡¨ç¤ºæ£€ç´¢æ‰€æœ‰è®ºæ–‡ï¼‰
            
        Returns:
            RAGé“¾
        """
        if not self.vector_store:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“")
        
        # å®šä¹‰æ£€ç´¢å™¨
        # æ³¨æ„ï¼šFAISSä¸æ”¯æŒmetadataè¿‡æ»¤ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨æ£€ç´¢åè¿‡æ»¤
        # å¦‚æœæŒ‡å®šäº†file_idï¼Œéœ€è¦æ£€ç´¢æ›´å¤šæ–‡æ¡£ç„¶åè¿‡æ»¤
        search_k = k * 10 if file_id else k  # å¦‚æœè¿‡æ»¤ï¼Œéœ€è¦æ£€ç´¢æ›´å¤šä»¥ç¡®ä¿æœ‰è¶³å¤Ÿç»“æœ
        retriever = self.vector_store.as_retriever(
            search_kwargs={"k": search_k}
        )
        
        # å®šä¹‰æç¤ºè¯æ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸–ç•Œçº§è®ºæ–‡ä¸“å®¶ï¼Œæ“…é•¿åˆ†æå’Œå›ç­”å­¦æœ¯è®ºæ–‡ç›¸å…³çš„é—®é¢˜ã€‚

è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ä½ æ— æ³•ä»æä¾›çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸–ç•Œçº§è®ºæ–‡ä¸“å®¶ã€‚"),
            ("user", template)
        ])
        
        # æ„å»ºRAGé“¾
        def filter_docs(docs):
            """è¿‡æ»¤æ–‡æ¡£ï¼ˆå¦‚æœæŒ‡å®šäº†file_idï¼‰"""
            if file_id:
                filtered = [doc for doc in docs if doc.metadata.get('file_id') == file_id]
                return filtered[:k] if filtered else []
            return docs[:k]
        
        def format_docs(docs):
            """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
            formatted = []
            for doc in docs:
                filename = doc.metadata.get('filename', 'æœªçŸ¥æ–‡æ¡£')
                library_name = doc.metadata.get('library_name', '')
                chunk_index = doc.metadata.get('chunk_index', 0)
                if library_name:
                    formatted.append(f"[æ¥æº: {library_name} - {filename}, ç‰‡æ®µ: {chunk_index+1}]\n{doc.page_content}")
                else:
                    formatted.append(f"[æ¥æºè®ºæ–‡: {filename}, ç‰‡æ®µ: {chunk_index+1}]\n{doc.page_content}")
            return "\n\n---\n\n".join(formatted)
        
        rag_chain = (
            {"context": retriever | filter_docs | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return rag_chain
    
    def query(self, question: str, k: int = 4, file_id: Optional[str] = None) -> str:
        """
        æŸ¥è¯¢RAGç³»ç»Ÿ
        
        Args:
            question: é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            file_id: å¦‚æœæŒ‡å®šï¼ŒåªæŸ¥è¯¢è¯¥æ–‡ä»¶çš„å†…å®¹ï¼ˆNoneè¡¨ç¤ºæŸ¥è¯¢æ•´ä¸ªæ•°æ®åº“ï¼‰
            
        Returns:
            å›ç­”
        """
        if not self.vector_store:
            return "âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“"
        
        try:
            rag_chain = self.create_rag_chain(k=k, file_id=file_id)
            response = rag_chain.invoke(question)
            return response
        except Exception as e:
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
    
    def query_with_sources(self, question: str, k: int = 4, file_id: Optional[str] = None) -> Dict:
        """
        æŸ¥è¯¢RAGç³»ç»Ÿå¹¶è¿”å›æ¥æºä¿¡æ¯
        
        Args:
            question: é—®é¢˜
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            file_id: å¦‚æœæŒ‡å®šï¼ŒåªæŸ¥è¯¢è¯¥æ–‡ä»¶çš„å†…å®¹ï¼ˆNoneè¡¨ç¤ºæŸ¥è¯¢æ•´ä¸ªæ•°æ®åº“ï¼‰
            
        Returns:
            åŒ…å«å›ç­”å’Œæ¥æºçš„å­—å…¸
        """
        if not self.vector_store:
            return {
                'answer': "âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“",
                'sources': []
            }
        
        try:
            # å¦‚æœæŒ‡å®šäº†file_idï¼ŒéªŒè¯æ˜¯å¦å­˜åœ¨
            if file_id:
                if file_id not in self.doc_metadata:
                    return {
                        'answer': f"âŒ é”™è¯¯: æŒ‡å®šçš„è®ºæ–‡ (file_id: {file_id}) ä¸åœ¨å‘é‡æ•°æ®åº“ä¸­",
                        'sources': [],
                        'paper_count': 0,
                        'query_scope': 'single_paper',
                        'error': 'file_not_found'
                    }
                
                # FAISSä¸æ”¯æŒmetadataè¿‡æ»¤ï¼Œæ£€ç´¢æ›´å¤šæ–‡æ¡£ç„¶åè¿‡æ»¤
                search_k = max(k * 20, 100)
                retriever = self.vector_store.as_retriever(search_kwargs={"k": search_k})
                all_docs = retriever.invoke(question)
                docs = [doc for doc in all_docs if doc.metadata.get('file_id') == file_id]
                
                if len(docs) < k:
                    search_k = max(search_k * 2, 200)
                    retriever = self.vector_store.as_retriever(search_kwargs={"k": search_k})
                    all_docs = retriever.invoke(question)
                    docs = [doc for doc in all_docs if doc.metadata.get('file_id') == file_id]
                
                if not docs:
                    return {
                        'answer': f"âŒ æ— æ³•ä»æŒ‡å®šè®ºæ–‡ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•ï¼š\n1. æ£€æŸ¥é—®é¢˜æ˜¯å¦ä¸è®ºæ–‡å†…å®¹ç›¸å…³\n2. å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\n3. ç¡®ä¿è®ºæ–‡å·²æ­£ç¡®åŠ è½½åˆ°å‘é‡æ•°æ®åº“",
                        'sources': [],
                        'paper_count': 0,
                        'query_scope': 'single_paper',
                        'error': 'no_matching_content'
                    }
                docs = docs[:k]
            else:
                retriever = self.vector_store.as_retriever(search_kwargs={"k": k})
                docs = retriever.invoke(question)
            
            # è·å–å›ç­”
            rag_chain = self.create_rag_chain(k=k, file_id=file_id)
            answer = rag_chain.invoke(question)
            
            # æ•´ç†æ¥æºä¿¡æ¯
            sources = []
            unique_papers = set()
            for doc in docs:
                paper_file_id = doc.metadata.get('file_id', '')
                if file_id and paper_file_id != file_id:
                    continue
                unique_papers.add(paper_file_id)
                sources.append({
                    'filename': doc.metadata.get('filename', 'æœªçŸ¥æ–‡æ¡£'),
                    'library_name': doc.metadata.get('library_name', ''),
                    'file_id': paper_file_id,
                    'chunk_index': doc.metadata.get('chunk_index', 0),
                    'content_preview': doc.page_content[:200] + "..."
                })
            
            return {
                'answer': answer,
                'sources': sources,
                'paper_count': len(unique_papers),
                'query_scope': 'single_paper' if file_id else 'all_papers'
            }
        except Exception as e:
            return {
                'answer': f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}",
                'sources': []
            }


def main(library_id: Optional[str] = None):
    """
    ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•
    
    Args:
        library_id: è®ºæ–‡åº“IDï¼Œå¦‚æœä¸ºNoneåˆ™äº¤äº’å¼é€‰æ‹©æˆ–ä½¿ç”¨é»˜è®¤å€¼
    """
    print("=" * 60)
    print("ğŸ“š è®ºæ–‡æ•°æ®åº“RAGæ£€ç´¢ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = PaperRAGSystem()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šlibrary_idï¼Œåˆ—å‡ºæ‰€æœ‰å¯ç”¨æ–‡åº“è®©ç”¨æˆ·é€‰æ‹©
    if library_id is None:
        print("\nğŸ“š å¯ç”¨çš„è®ºæ–‡åº“:")
        libraries = rag_system.list_libraries()
        
        if not libraries:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡åº“ï¼Œä½¿ç”¨é»˜è®¤åº“ 'default'")
            library_id = "default"
        else:
            print("\nè¯·é€‰æ‹©è¦ä½¿ç”¨çš„è®ºæ–‡åº“:")
            for i, lib in enumerate(libraries, 1):
                print(f"  {i}. {lib.get('display_name', lib['id'])} (ID: {lib['id']})")
                if lib.get('description'):
                    print(f"     æè¿°: {lib['description']}")
            
            try:
                choice = input("\nè¯·è¾“å…¥åºå· (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤åº“ 'default'): ").strip()
                if choice:
                    choice_idx = int(choice) - 1
                    if 0 <= choice_idx < len(libraries):
                        library_id = libraries[choice_idx]['id']
                        print(f"âœ… å·²é€‰æ‹©: {libraries[choice_idx].get('display_name', library_id)}")
                    else:
                        print("âš ï¸ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤åº“ 'default'")
                        library_id = "default"
                else:
                    library_id = "default"
                    print("âœ… ä½¿ç”¨é»˜è®¤åº“ 'default'")
            except (ValueError, KeyboardInterrupt):
                print("\nâš ï¸ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åº“ 'default'")
                library_id = "default"
    
    # æ–¹å¼1: ä»æ–‡åº“åŠ è½½è®ºæ–‡å¹¶æ„å»ºå‘é‡æ•°æ®åº“
    print(f"\n1ï¸âƒ£ ä»æ–‡åº“åŠ è½½è®ºæ–‡ (åº“ID: {library_id})...")
    papers = rag_system.load_papers_from_library(library_id=library_id)
    
    if papers:
        print("\n2ï¸âƒ£ æ„å»ºå‘é‡æ•°æ®åº“...")
        success = rag_system.build_vector_store(papers, library_id=library_id)
        
        if not success:
            print(f"\nğŸ”„ å°è¯•åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ (åº“ID: {library_id})...")
            rag_system.load_vector_store(library_id=library_id)
    else:
        print(f"\nğŸ”„ æœªæ‰¾åˆ°è®ºæ–‡ï¼Œå°è¯•åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ (åº“ID: {library_id})...")
        rag_system.load_vector_store(library_id=library_id)
    
    # æ–¹å¼2: ç›´æ¥æŸ¥è¯¢ï¼ˆå¦‚æœå‘é‡æ•°æ®åº“å·²å­˜åœ¨ï¼‰
    if rag_system.vector_store:
        print("\n3ï¸âƒ£ å¼€å§‹æŸ¥è¯¢...")
        print("-" * 60)
        
        # ç¤ºä¾‹æŸ¥è¯¢
        questions = [
            "agentæœ€ç«çš„è®ºæ–‡æ˜¯å“ªä¸ªï¼Ÿ",
            "è®ºæ–‡ä¸­æåˆ°äº†å“ªäº›å…³é”®æŠ€æœ¯ï¼Ÿ",
            "æ€»ç»“ä¸€ä¸‹è®ºæ–‡çš„ä¸»è¦è´¡çŒ®"
        ]
        
        for question in questions:
            print(f"\nâ“ é—®é¢˜: {question}")
            print("-" * 60)
            
            # æŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœï¼ˆæŸ¥è¯¢æ•´ä¸ªæ•°æ®åº“ï¼‰
            result = rag_system.query_with_sources(question, k=3)
            
            print(f"ğŸ“Š æŸ¥è¯¢èŒƒå›´: {'å•ç¯‡è®ºæ–‡' if result.get('query_scope') == 'single_paper' else 'æ•´ä¸ªè®ºæ–‡æ•°æ®åº“'}")
            if result.get('paper_count'):
                print(f"ğŸ“„ æ¶‰åŠè®ºæ–‡æ•°: {result['paper_count']} ç¯‡")
            
            print(f"\nğŸ’¡ å›ç­”:\n{result['answer']}")
            
            if result['sources']:
                print(f"\nğŸ“š æ¥æºç‰‡æ®µ ({len(result['sources'])} ä¸ª):")
                for i, source in enumerate(result['sources'], 1):
                    library_name = source.get('library_name', '')
                    if library_name:
                        print(f"  {i}. [{library_name}] {source['filename']} (ç‰‡æ®µ {source['chunk_index']+1})")
                    else:
                        print(f"  {i}. {source['filename']} (ç‰‡æ®µ {source['chunk_index']+1})")
                    print(f"     é¢„è§ˆ: {source['content_preview']}")
            
            print("\n" + "=" * 60)
        
        # ç¤ºä¾‹ï¼šæŸ¥è¯¢å•ç¯‡è®ºæ–‡
        if rag_system.doc_metadata:
            print("\n" + "=" * 60)
            print("ğŸ“ ç¤ºä¾‹ï¼šæŸ¥è¯¢å•ç¯‡è®ºæ–‡")
            print("=" * 60)
            
            # è·å–ç¬¬ä¸€ç¯‡è®ºæ–‡çš„file_id
            first_file_id = list(rag_system.doc_metadata.keys())[0]
            first_filename = rag_system.doc_metadata[first_file_id]['filename']
            
            print(f"\nğŸ“„ æŸ¥è¯¢è®ºæ–‡: {first_filename}")
            print(f"ğŸ“‹ File ID: {first_file_id}")
            print("-" * 60)
            
            question = "è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ"
            print(f"\nâ“ é—®é¢˜: {question}")
            
            result = rag_system.query_with_sources(question, k=3, file_id=first_file_id)
            
            print(f"ğŸ“Š æŸ¥è¯¢èŒƒå›´: å•ç¯‡è®ºæ–‡ ({first_filename})")
            print(f"\nğŸ’¡ å›ç­”:\n{result['answer']}")
            
            if result['sources']:
                print(f"\nğŸ“š æ¥æºç‰‡æ®µ ({len(result['sources'])} ä¸ª):")
                for i, source in enumerate(result['sources'], 1):
                    library_name = source.get('library_name', '')
                    if library_name:
                        print(f"  {i}. [{library_name}] {source['filename']} (ç‰‡æ®µ {source['chunk_index']+1})")
                    else:
                        print(f"  {i}. {source['filename']} (ç‰‡æ®µ {source['chunk_index']+1})")
    else:
        print("\nâŒ æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥é…ç½®")


if __name__ == "__main__":
    import sys
    
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ä¼ å…¥library_id
    # ç”¨æ³•: python test_rag.py library_d8fd90da
    library_id = None
    if len(sys.argv) > 1:
        library_id = sys.argv[1]
        print(f"ğŸ“‹ ä»å‘½ä»¤è¡Œå‚æ•°è·å–åº“ID: {library_id}")
    
    main(library_id=library_id)


